\section{Анализ методов обучения с подкреплением}
Для решения задачи синтеза алгоритма управления был выбран подход с использованием обучения с подкреплением.
Алгоритмы обучения с подкреплением делятся на две группы:
\begin{itemize}
	\item с аналоговым выходом;
	\item с дискретным выходом.
\end{itemize}

Данные группы алгоритмов решают разные задачи: первые используются в системах, где управляющий сигнал представлен диапазоном значений, например, управление напряжением двигателя. 
Вторая группа алгоритмов используется в системах, где управляющий сигнал представлен конечным набором действий. 

В этой главе будут рассмотрены 3 алгоритма с аналоговым выходом:
\begin{itemize}
	\item DDPG;
	\item PPO;
	\item TRPO.
\end{itemize}

А также 4 алгоритма с дискретным выходом:
\begin{itemize}
	\item CEM;
	\item DQN;
	\item DUEL DQN;
	\item SARSA.
\end{itemize}

Вышеупомянутые алгоритмы сравнены между собой по критериям следующим критериям:
\begin{itemize}
	\item скорость обучения;
	\item качество;
	\item стабильность;
	\item количество обучаемый параметров.
\end{itemize}

\subsection{CEM}

Метод перекрестной энтропии (CEM) -- общий алгоритм для решения глобальной оптимизационной задачи. 
Данный метод позволяет обучить нейронную сеть для решения задачи обучения с подкреплением. 
Нейронная сеть является управляющим звеном, на нее поступают выходы агента, например, показания датчиков, а выходом нейронной сети являются входы агента, то есть стратегия управления.

На практике стратегия представляется, как распределения вероятности над действиями. 
Данный подход очень похож с задачей классификации: выбор одного действия из набора. 
Алгоритм пропускает через нейронную сеть наблюдение окружающей среды, забирает вероятностное распределение над действиями (стратегию) и генерирует случайную выборку из этого распределения. 
Это случайная выборка добавляет неопределенности агенту, что хорошо влияет на начало обучающего процесса. Чем дольше происходит процесс обучения, тем меньше становится неопределенность. 
После действия, агент получает награду и цикл повторяется \cite{CEM}.

В процессе обучения мы минимизируем разницу между начальной стратегией и оптимальной, найденной методом выборки по значимости \cite{CEM1}, в итеративном процессе. 
Разность между текущей и оптимальной стратегией определяется дивергенцией Кульбака-Лейблера -- формула (\ref{ fraq1 }).

\begin{equation}\label{ fraq1 }
\displaystyle D_{\mathrm {KL} }(P\|Q)=\int _{X}p\,\log {\frac {p}{q}}\,{\rm {d}}\mu 
\end{equation}

Где p(x), q(x) -- функции плотности распределения, сравниваемых величин.

Достоинства данного метода:
\begin{itemize}
	\item хорошо работает в легких средах;
	\item робастный к изменению гиперпарамтеров.
\end{itemize}

Недостатки CEM:
\begin{itemize}
	\item низкая скорость обучения.
\end{itemize}

\subsection{DQN}

Deep Q – Network (DQN) – улучшение классического алгоритма обучения с подкреплением Q-learning за счёт использования нейронных сетей.

Q-learning (в русскоязычной литературе иногда употребляется Q-обучение) – алгоритм, согласно которому агент на основе получаемого от среды вознаграждения формирует функцию полезности Q, что впоследствии дает ему возможность уже не случайно выбирать стратегию поведения, а учитывать опыт предыдущего взаимодействия со средой. 
Одно из преимуществ Q-обучения -- то, что оно в состоянии сравнить ожидаемую полезность доступных действий, не формируя модели окружающей среды. 
Применяется для ситуаций, которые можно представить в виде Марковского процесса принятия решений (англ. Markov decision process (MDP)), то есть процесса, в котором вероятности переходов между состояниями не зависят от истории предыдущих переходов. 

Алгоритм Q-learning пытается получить значение функции Q в явном виде для всех возможных пар «состояние среды - действие», чтобы потом максимизировать ожидаемый результат, найдя оптимальную стратегию. 
Однако даже в самых простых задачах робототехники число состояний среды очень велико, а если его умножить на количество возможных действий, то получится таблица, обрабатывать которую на борту робота будет затруднительно. 
Поэтому в алгоритме DQN таблица Q заменена функцией, которую приближает свёрточная нейронная сеть. 
Помимо этого, у DQN есть ещё несколько усовершенствований по сравнению с классическим алгоритмом Q-learning. 
Во-первых, практика показывает, что обучаться непосредственно на состояниях, происходящих друг за другом, -- плохой подход, так как такие состояния слишком похожи друг на друга, сильно коррелируют, причём со временем их распределение, естественно, сдвигается в зависимости от действий (или, например, положения) робота, но остается локализованным. 
Это мешает эффективному обучению, ведь в обычной постановке задачи обучения предполагается, что тренировочные данные независимы, а распределение данных со временем не меняется. 
Поэтому по мере обучения DQN сначала накапливает некоторый опыт, сохраняя свои действия и их результаты на протяжении какого-то времени, а потом выбирает из этого опыта случайную выборку (mini-batch) отдельных примеров для обучения, взятых в случайном порядке. 
Во-вторых, важную роль для успеха сыграло то, что при обучении DQN сеть, которая отвечала за целевую функцию, была отделена от обучающейся сети. 
Сделано это по причине того, что на практике нейронные сети довольно быстро заходят в локальные экстремумы и начинают очень глубоко исследовать глобально неважные части пространства поиска, бесцельно тратя ресурсы и фактически не обучаясь. 
Избежать этого можно, сделав так, чтобы сеть не сразу использовала обновленную версию в целевой функции, а обучалась достаточно долгое время по старым образцам, прежде чем использовать обновлённую целевую функцию. 
В-третьих, на практике обычно применяется архитектура, в которой возможное действие агента не подается на вход, а просто у сети столько выходов, сколько возможных действий, и, получая на входе состояние, сеть пытается предсказать результаты полезности, ценности каждого действия (после чего, естественно, выбирает максимальный). 
Это важное улучшение, потому что оно позволяет получить ответы сразу для всех действий за один проход по сети, что ускоряет происходящее в разы, а сеть от этого сильно сложнее не становится, ведь основная часть ее «логики» остается прежней и используется заново.

Преимущества:
\begin{itemize}
	\item простой относительно других алгоритмов глубокого обучения с подкреплением, в следствие чего обучается быстрее более сложных алгоритмов;
	\item благодаря усовершенствованиям справляется с проблемой локальных минимумов.
\end{itemize}

Недостатки:
\begin{itemize}
	\item не использует всю историю обучения;
	\item может ограничивать скорость обучения.
\end{itemize}


\subsection{DUEL DQN}
Duel DQN является улучшением алгоритма DQN за счёт разделения Q-сети на два канала, один из которых, вычисляет оценку позиции V, которая является функцией только состояния среды, а другой -- зависящую от действия функцию преимущества (advantage function) A. 
На последнем этапе они просто складываются в прежнюю функцию Q. 
Таким образом, одна часть сети обучается оценивать позицию как таковую, а другая -- предсказывать, насколько полезны будут разные наши действия в этой позиции. 
Такое небольшое изменение в архитектуре сети приближает смысл оценки к естественному, что часто существенно улучшает результаты \cite{DUEL_DQN}.
Сравнение архитектур DQN и DUEL DQN показана на рисунке \ref{dueldqn}.

\addimghere{dueldqn}{0.6}{Сравнение архитектур DQN(сверху) и DUEL DQN(снизу).}{dueldqn} 

\subsection{SARSA}

Алгоритм SARSA – схож с алгоритмом Q-learning. 
Данный алгоритм также выбирает действие у которого будет максимальное значение Q. 
Главным отличием является то, что SARSA вычисляет значение Q используя текущую стратегию, а Q-learning жадный поиск. 
Данное различие позволяет использовать этот алгоритм для обучения нейросети на реальном дорогостоящем роботе, так как каждое выбранное действие не будет кардинально отличаться от предыдущего.

Достоинства:
\begin{itemize}
	\item учитывает штрафы от предыдущих действий.
\end{itemize}

Недостатки:
\begin{itemize}
	\item может игнорировать оптимальную стратегию;
	\item учиться дольше чем Q-learning.
\end{itemize}

\subsection{DDPG}

В алгоритмах Q-learning, DQN, Duel DQN и SARSA действия выбираются согласно максимальному Q-значению. В таких алгоритмах (они называются value-based, то есть основанных на значениях функции Q) стратегия представляет собой не что иное, как выбор действия с максимальным значением Q. Проблема value-based подхода заключается в необходимости вычисления оценки награды для каждого возможного действия в каждом возможном состоянии. Особенно остро эта проблема встаёт, когда дело касается бесконечного пространства действий (обычно это физические задачи, например, классическая задача балансировки обратного маятника. Также это задачи управления вращением колёс, движением манипулятора). Например, DQN создавался для решения проблемы с достаточно высокоразмерным пространством состояний (пиксели на экране в играх Atari), но при этом очень малоразмерным дискретным пространством действий (до двух десятков комбинаций кнопок). Но задачи физического контроля имеют непрерывные и высокоразмерные пространства действий. DQN не может быть непосредственно применён к непрерывным пространствам даже при грубой дискретизации пространства действий, так как размер выходного слоя нейронной сети Q-network растёт экспоненциально с ростом степеней свободы агента \cite{Nikolenko}.

В алгоритмах градиентного спуска по стратегиям (policy gradient (PG) methods) вместо обучения функции Q, возвращающей ожидаемую ценность каждого действия в каждом состоянии, напрямую обучается стратегия, которая состоянию ставит в соответствие действие. В данном семействе алгоритмов центральную роль занимает стратегия, согласно которой агент совершает действие в зависимости от текущего состояния и от параметров самой стратегии (обычно это веса нейронной сети). Задача обучения с подкреплением заключается в максимизации целевой функции. Вознаграждение, которое напрямую влияет на значение целевой функции, зависит от стратегии, которая в свою очередь зависит от своих параметров, следовательно, сама целевая функция зависит от параметров стратегии. PG, который является на данный момент центральным методом обучения с подкреплением в задачах робототехники, базируется на идее оптимизации целевой функции методом градиентного спуска по стратегиям вместо построения моделей окружающей среды или оценки функции Q \cite{pgms}.

Такой подход работает не в каждой задаче, так как, используя разумные ресурсы, можно посчитать градиент только за ограниченное количество шагов и с ограниченной точностью. Однако благодаря policy gradient theorem производную можно вычислять для более продолжительных промежутков, например, для целой траектории. На наборах траекторий и суммарной награды по итогу прохождения соответствующих траекторий обучаются параметры стратегии.

Стратегия бывает двух видов: детерминированная и стохастическая. Детерминированная состоянию ставит в соответствие действие, которое необходимо в этом состоянии выполнить. Детерминированные стратегии используются в детерминированных средах или в средах, где случайные события слабо влияют на поведение агента (например, движение исправного робота в лабиринте при стабильных погодных условиях и при однородном освещении). Стохастические стратегии состоянию среды ставят в соответствие распределение над действиями. Это означает, что разные действия могут быть выбраны для одного и того же состояния. Такие стратегии используются в средах, где доля случайных или неопределённых событий значительна, то есть почти в любой в естественной среде. Детерминированные стратегии проще описываются и обучаются, поэтому алгоритмы deterministic policy gradient (DPG) более популярны, нежели stochastic policy gradient (SPG).

На основе идей, принёсших успех DQN, был создан алгоритм deep deterministic policy gradient (DDPG) \cite{ddpg}, применимый для непрерывного пространства действий. Также важной чертой DDPG является применение архитектуры «actor-critic» (устоявшегося русского названия не существует, но приблизительный перевод – «действующее лицо-критик»), которая является по сути объединением идей Q-learning и policy gradient, показано на рисунке \ref{ddpg}. 
\addimghere{ddpg}{0.6}{Архитектура «actor-critic».}{ddpg} 

В данном алгоритме обучаются две нейронные сети: сеть-actor обучает параметры стратегии, чтобы поданному на вход состоянию ставилось в соответствие лучшее действие, и сеть-critic, обучающаяся оцениванию стратегии по ошибке, которая высчитывается по тому же правилу, что и при обновлении Q-функции в Q-learning. Но при этом функция оценки используется только для оптимизации параметров стратегии, но сама при этом при выборе действия не используется. 

Преимущества алгоритма DDPG:
\begin{itemize}
	\item Алгоритмам, основанным на оптимизации функции оценки, присущи большие колебания во время обучения. Эти колебания связаны с тем, что выбор действия может резко измениться при сколь угодно малых изменениях оценки действий. В алгоритмах, основанных на оптимизации стратегии, ищется градиент по стратегии, поэтому процесс обучения проходит более гладко.
	\item Возможность рассмотрения проблемы исследования (problem of exploration) независимо от алгоритма DDPG.
	\item Возможность работать с высокоразмерными непрерывными пространствами действий.
\end{itemize}

Недостатки алгоритма DDPG:
\begin{itemize}
	\item Вычисление градиента по стратегии приводит к возможной сходимости в локальном максимуме вместо глобального. Такая проблема отсутствует в value-based подходе, где постоянно ищется лучшее из возможных действий.
	\item В случае отсутствия добавленной извне формализации исследования среды, алгоритм не будет исследовать среду вообще.
\end{itemize}

\subsection{TRPO}
Метод региона доверия (TRPO) -- градиентный метод оптимизации, позволяющий напрямую изменять стратегию. 
Для нахождения оптимальной стратегии, необходимо максимизировать функцию наград. 
На качество оценки ожидаемого значения функции наград с текущей стратегией влияет разность между текущей стратегией и предыдущей. 
Если стратегия сильно отличается от предыдущей, то точность будет ниже. 
Для увеличения точности вводится понятие региона доверия, в котором гарантировано качество стратегии будет не ухудшаться. 
Оптимальная стратегия ищется итеративным процессом, каждый раз выбирается из региона доверия \cite{trpo}.

Для гарантии не ухудшения стратегии используется итеративный подход минимальная-максимизация \cite{trpo1}.
Регионом доверия является регион с определенным радиусом, в котором ищется оптимальная точка. 
Радиус может меняться от шага к шагу. Для нахождения градиента с использованием предыдущих данных используется выборка по значимости.

Достоинства данного метода:
\begin{itemize}
	\item легкая масштабируемость,
	\item стабильность
\end{itemize}

Недостатки данного метода:
\begin{itemize}
	\item Тяжело использовать в случаях большого количества выходов.
\end{itemize}


\subsection{PPO}

Proximal Policy Optimisation (PPO) -- метод обучения для задачи обучения с подкреплением, на каждом шаге максимизируя функцию наград, при этом не сильно изменяя стратегию. 
Данный метод схож с методом TRPO, но есть и отличия.
Во-первых, для вычисления второй производной, метод TRPO вычисляет приближенное значение второй производной, а  метод PPO  заменяет на первую производную, за счёт введения гиперпараметров. 
Также для нахождения новой стратегии в регионе доверия, используется дивергенция Кульбака-Лейблера между текущей стратегией и новой \cite{ppo}.
Достоинства данного метода:
\begin{itemize}
	\item хорошо работает с большим количеством выходов.
\end{itemize}

Недостатки:
\begin{itemize}
	\item менее устойчивый чем TRPO.
\end{itemize} 



\subsection{Эксперименты}
Для сравнения алгоритмов обучения с подкреплением были проведены эксперименты, отдельно для дискретного управления и отдельно для аналогового. 
Для дискретного управления решается задача равновесия обратного маятника, а для аналогового управления решается задача равновесия двойного обратного маятника. 
Выбранные задачи обусловлены наличием готовых примеров. 
Симуляция алгоритмов происходит в среде Mujoco \cite{mujoco}. Эта среда выбрана, благодаря быстрой обработке кинематики и динамики объектов.
Оба алгоритма будут сравнены по одинаковым критериям. 
Скорость обучения -- определяется по эпизоду, на котором алгоритм достигает максимальной награды. 
Чем меньше это значение, тем быстрее алгоритм обучается. 
Лучший результат определяет качество работы алгоритма. 
Чем выше значение данного критерия, тем лучше качество. 
Стабильность оценивается по графику зависимости шага от награды, зависимость стабильная, если не происходит переобучения с увеличением продолжительности обучения. 
На рисунке \ref{experiment1}, слева и справа показаны полученные зависимости награды от шага, для аналогового и дискретного управления соотвественно. 
Количество обучаемых параметров -- это количество свободных параметров нейронной сети. 

\addimghere{experiment1}{1}{Зависимость награды от номера эпизода для аналогового и дискретного управления.}{experiment1} 

\subsection{Вывод по главе}
По результатам экспериментов составлена сравнительная \mbox{таблица \ref{table:RLdifference}.}

\begin{table}[H]
	\caption{Сравнение алгоритмов обучения с подкреплением}\label{table:RLdifference}
	\begin{tabular}{|m{2.5cm}|m{1.9cm}|m{2cm}|m{3cm}|m{2.5cm}|m{2.5cm}|}
		\hline Алгоритм & Скорость обучения & Лучший результат & Стабильность &  Количество обучаемых параметров & Тип управления \\
		\hline CEM       & 2500 & 35   & Да  & 658  & Дискретное \\
		\hline DQN       & 80   & 200  & Да  & 658  & Дискретное \\
		\hline DUEL DQN  & 140  & 200  & Да  & 658  & Дискретное \\
		\hline SARSA     & 450  & 160  & Нет & 658  & Дискретное \\
		\hline DDPG      & 1500 & 2800 & Нет & 9000 & Аналоговое \\
		\hline PPO       & 4200 & 7500 & Да  & 4485 & Аналоговое \\
		\hline TRPO      & 3000 & 6200 & Да  & 4485 & Аналоговое \\
		\hline 
	\end{tabular}
\end{table}

Из полученных результатов, лучшими методами обучения для дискретного управления является DQN и Duel DQN. 
Алгоритмы достигли лучшего результата в 200 шагов, что является максимальным в данном эксперименте, также эти алгоритмы стабильные в отличие от алгоритма SARSA.  
Особенностью алгоритма SARSA является небольшое отклонение новой стратегии от предыдущей, а так как количество состояний движения было всего 2, то заданный алгоритм пытался уравновесить маятник, используя только движение в одну из сторон.
Алгоритм DQN обучился быстрее, ему потребовалось на 80 эпизодов меньше, чем Duel DQN.

Для аналогового управления самую большую скорость обучения показал алгоритм DDPG, но его максимальный результат в 2,2 раза меньше, чем результат TRPO, и в 2,7 раза меньше, чем результат PPO.
Алгоритмы PPO и TRPO не были переобучены за 4500 эпизодов, в то время как DDPG начал переобучаться сразу после своего максимального значения, после 1500 эпизодов. 
Лучшими алгоритмами для аналогового управления являются алгоритмы PPO и TRPO, если основным является критерий качества, а если главным является скорость обучения, то DDPG.

\clearpage
