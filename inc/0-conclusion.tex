\anonsection{Заключение}
В данной работе были рассмотрены подходы к созданию алгоритмов управления шагающими роботами, с целью сравнения и выбора лучшего для решения следующих проблем:
\begin{itemize}
	\item уменьшение сложности разработки алгоритма;
	\item увеличение устойчивости разработанного алгоритма к новым внешним условиям;
\end{itemize}

Были рассмотрены наиболее актуальные подходы для решения задачи управления: подход на основе нечеткой логики, эволюционный подход, подход на основе обучения с подкреплением, экспертный подход. 
Результаты сравнения представлены в \mbox{таблице \ref{table:ApDifference}.}
Степень формализации -- основной критерий, он показывает количество начальной информации, которую необходимо знать об агенте, и как полно необходимо описать кинематику и динамику агента.
Лучшим подходом в данном сравнение является алгоритм на основе обучения с подкреплением.

Для выбранного подхода были рассмотрены алгоритмы обучения для дискретного и аналогово выхода.
С аналоговым выходом:
\begin{itemize}
	\item DDPG;
	\item PPO;
	\item TRPO.
\end{itemize}

С дискретным выходом:
\begin{itemize}
	\item CEM;
	\item DQN;
	\item DUEL DQN;
	\item SARSA.
\end{itemize}

Для сравнения алгоритмов обучения с подкреплением были проведены эксперименты, отдельно для дискретного управления и отдельно для аналогово. 
Для дискретного управления решается задача равновесия обратного маятника, а для аналогово управления решается задача равновесия двойного обратного маятника. 
Выбранные задачи обусловлены наличием готовых примеров. 
Симуляция алгоритмов происходит в среде Mujoco \cite{mujoco}. Данная среда выбрана, благодаря быстрой обработке кинематики и динамики объектов.

Данные алгоритмы были сравнены между собой по критериям скорость обучения, качество, стабильность, количество обучаемый параметров.
Результаты сравнения приведены в \mbox{таблица \ref{table:RLdifference}.}
Из полученных результатов лучшими методами обучения для дискретного управления являются DQN и Duel DQN.
Лучшими алгоритмами для аналогового управления являются алгоритмы PPO и TRPO, если основным является критерий качества, а если главным является скорость обучения, то DDPG.

Для дальнейшей работы необходимо создать модель шагающего робота и протестировать выбранные алгоритмы обучения на ней. 
Целью будет являться нахождения лучшего алгоритма для задач перемещения.
\clearpage
